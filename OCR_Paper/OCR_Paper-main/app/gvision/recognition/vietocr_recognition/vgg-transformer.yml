# for training
# pretrain: https://vocr.vn/data/vietocr/vgg_transformer.pth

# url or local path (predict)
weights: ./app/gvision/weights/common_model/vietocr_model/vgg_transformer.pth
device: cuda:0
vocab: 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!"#$%&''()*+,-./:;<=>?@[\]^_`{|}~ '
seq_modeling: transformer
transformer:  
    d_model: 256
    nhead: 8
    num_encoder_layers: 6
    num_decoder_layers: 6
    dim_feedforward: 2048
    max_seq_length: 1024
    pos_dropout: 0.1
    trans_dropout: 0.1

dataset:    
    # name of your dataset
    name: data
    # path to annotation and image
    data_root: ./img/
    train_annotation: annotation_train.txt
    valid_annotation: annotation_val_small.txt
    # resize image to 32 height, larger height will increase accuracy
    image_height: 32
    image_min_width: 32
    image_max_width: 512
predictor:
    # disable or enable beamsearch while prediction, use beamsearch will be slower
    beamsearch: False

backbone: vgg19_bn
cnn:
    pretrained: False
    # pooling stride size
    ss:
        - [2, 2]
        - [2, 2]
        - [2, 1]
        - [2, 1]
        - [1, 1]         
    # pooling kernel size 
    ks:
        - [2, 2]
        - [2, 2]
        - [2, 1]
        - [2, 1]
        - [1, 1]
    # dim of ouput feature map
    hidden: 256